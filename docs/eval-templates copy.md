# 现有的评估模板

在使用 Evals 的过程中，我们发现了几个可以适应多种基准测试的"模板"。我们已经在 `evals/elsuite` 中实现了这些模板，以简化新评估的开发。我们相信，有了这些模板，许多评估将不需要编写任何代码就能实现！相反，你只需选择一个现有模板，并简单地指定数据集和参数即可。

## 基础评估模板

在期望的模型响应变化很小的情况下，比如回答多选题或有直接答案的简单问题时，我们发现以下模板非常有用。

对于模型完成输出 `a` 和正确答案列表 `B`，以下评估实现了：
- [`basic/match.py:Match`](../evals/elsuite/basic/match.py): `any([a.startswith(b) for b in B])`
- [`basic/includes.py:Includes`](../evals/elsuite/basic/includes.py): `any([(b in a) for b in B])`
- [`basic/fuzzy_match.py:FuzzyMatch`](../evals/elsuite/basic/fuzzy_match.py): `any([(a in b or b in a) for b in B])`

要比较 *JSON 格式* 的模型完成输出 `a` 与同样以 JSON 格式表示的正确答案列表 `B`，请使用以下评估：
- [`basic/json_match.py:JsonMatch`](../evals/elsuite/basic/json_match.py) 如果 `a` 与 `B` 中至少一个答案完全相同，则视为匹配。如果两个 JSON 对象具有相同的键集合且每个键的值都相同，则它们是相同的。键的顺序不重要，值之外的空白会被忽略。无效的 JSON 永远不会匹配。

你使用哪个评估模板将取决于你的具体用例。我们始终建议你检查模型的完成输出，因为这将帮助你确定如何以及是否需要调整你的提示（或参考答案）并选择评估模板。学术基准测试通常符合这些基本评估的模式，我们在 `examples` 文件夹中以 Jupyter 笔记本的形式实现了几个端到端的学术评估示例。

有时，[自定义评估逻辑](custom-eval.md)可能更适合你的需求。一个例子是[机器翻译](../evals/elsuite/translate.py)评估[示例](../examples/lafand-mt.ipynb)，其中有一个我们希望在评估中使用的独特且明确定义的指标。在决定使用自定义评估逻辑、使用基本评估模板或使用下面描述的模型评分评估时，你应该运用你的最佳判断。

## 模型评分评估模板

在期望的模型响应可能包含显著变化的情况下，比如回答开放式问题时，我们发现使用模型对自己进行评分是一种可行的自动评估策略。一般来说，评估模型和被评估的模型不必相同，尽管为了便于解释，我们在这里假设它们是相同的。

[`modelgraded/classify.py:ModelBasedClassify`](../evals/elsuite/modelgraded/classify.py) 实现了我们的模型评分评估模板的主要逻辑。简而言之，我们获取模型对原始提示的完成输出，将其包装在评估提示中，然后获取模型对评估提示的完成输出，我们将其解析为我们感兴趣的指标。关键是，评估提示应该引导模型以易于解析的方式回答，例如，以多选格式或简单的是/否回答。我们在下面描述一些模型评分评估的示例，但首先我们要指定这个评估模板的参数。

### 模型评分评估的参数

参考 [`classify.py:ModelBasedClassify`](../evals/elsuite/modelgraded/classify.py) 类以了解这些参数在代码中的使用方式。

- `prompt`：评估提示，它应该接收模型对原始提示的完成输出，可能还包括一些其他信息，并引导模型提供易于解析的评估。用大括号标记的部分（即 `{key}`）从数据 `input_outputs` 或额外的 `args` 中填充（见下文）。
- `input_outputs`：指定使用哪些输入生成哪些完成输出的映射。对于许多评估来说，只会有一个输入-完成对，但也可能有更多，例如，当比较两个完成输出时。
- `choice_strings`：给定评估提示时，我们期望模型完成输出包含的选项。例如，`"ABCDE"` 或 `["Yes", "No", "Unsure"]`。模型返回的任何其他选项都会被解析为 `"__invalid__"`。
- `choice_scores`（可选）：每个选项到其分数的映射，作为指标记录。例如，如果 `"Yes"` 响应（相应地 `"No"`）表示模型的原始完成输出是好的（相应地坏的），我们可能会给这个选项分配 1 分（相应地 0 分）。
- `eval_type`（可选）：我们期望模型如何格式化其对评估提示的响应。目前支持的选项有：
  - `"cot_classify"`（"思维链后分类"，即先推理后回答）期望可解析部分（即包含选项的部分）将在完成输出的末尾。我们推荐这个作为默认选项，因为它通常提供最准确的模型评分评估。
  - `"classify_cot"`（先回答后推理）期望模型响应将首先包含选项。
  - `"classify"` 期望模型响应只包含选项。

  有两种方式指定 `eval_type`。推荐的方式是在 `evals/registry/evals` YAML 文件中指定。如果这样做，一条指令将自动附加到 `prompt` 以引导模型采用预期的格式（参见[代码](../evals/elsuite/modelgraded/classify.py)中的 `ANSWER_PROMPTS`）。或者，你可以在 `evals/registry/modelgraded` YAML 中指定 `eval_type`，但你需要在 `prompt` 中直接包含适当的指令。
- `output_template`（可选）：如果指定，决定模型的输出（或如果 `n > 1` 时的多个输出）将如何在完成输出中格式化。

### 模型评分评估示例

要实例化模型评分评估，在 `evals/registry/modelgraded` 中创建一个 YAML 文件，指定上述参数的值。我们提供了一些示例，这些示例不仅说明了创建模型评分评估的过程，而且我们认为它们足够通用，可以直接用于许多评估。

[`fact.yaml`](../evals/registry/modelgraded/fact.yaml)：一个事实一致性评估，给定完成输出 `a` 和参考答案 `b`，返回：
- `"A"` 如果 `a` $\subseteq$ `b`，即提交的答案是专家答案的子集且与之完全一致。
- `"B"` 如果 `a` $\supseteq$ `b`，即提交的答案是专家答案的超集且与之完全一致。
- `"C"` 如果 `a` $=$ `b`，即提交的答案包含与专家答案相同的所有细节。
- `"D"` 如果 `a` $\neq$ `b`，即提交的答案与专家答案之间存在分歧。
- `"E"` 如果 `a` $\approx$ `b`，即答案有所不同，但这些差异从事实性的角度来看并不重要。

[`closedqa.yaml`](../evals/registry/modelgraded/closedqa.yaml)：一个问答评估，给定包含问题和回答问题所需信息的提示，检查模型的答案是否：
- 相关，即从提示中提供的信息中提取，
- 简洁，即不包含不必要的细节或信息，以及
- 正确，即使用提取的信息得出正确的结论。

注意，这个评估更一般地实现为一个"标准检查"评估，它将评估提示指定为检查给定标准，并逐一输入上述要求。我们相信，通过指定详细说明感兴趣标准的"评分标准"并遵循相同的提示和是/否选项，可以实现许多其他评估。

[`battle.yaml`](../evals/registry/modelgraded/battle.yaml)：一个头对头评估，比较两个可能不同提示的两个模型完成输出。这里使用 `choice_scores` 来记录第一个完成输出被判断为比第二个更好的频率。

我们包含了测试更具体模型能力（如幽默）的其他示例，这些示例不太容易推广到其他评估。然而，这些示例仍然有助于说明编写评估提示和设置模型评分评估的不同方式。有关构建模型评分评估的更详细步骤，请参见[本节](build-eval.md#for-model-graded-evals-a-step-by-step-workflow)。
