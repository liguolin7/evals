\appendix
\section{Code Listings}

\subsection{Core Components}
The framework consists of several key Python modules:

\begin{itemize}
    \item \textbf{eval.py} - Core evaluation implementation:
    \begin{lstlisting}[language=Python, breaklines=true, basicstyle=\ttfamily\scriptsize]
class FaithfulnessEval(Eval):
    METRICS = [
        "factual_accuracy",
        "logical_coherence",
        "context_relevance",
        "interpretative_reasoning",
        "information_completeness",
        "hallucination_score"
    ]
    
    TYPE_WEIGHTS = {
        "general": {
            "factual_accuracy": 0.3,
            "logical_coherence": 0.2,
            "context_relevance": 0.15,
            "interpretative_reasoning": 0.15,
            "information_completeness": 0.1,
            "hallucination_score": 0.1
        }
    }
    
    def eval_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        context = sample.get("context", "")
        query = sample.get("query", "")
        reference = sample.get("reference", "")
        sample_type = sample.get("type", "general")
        
        metrics = {
            "factual_accuracy": self.metrics.calculate_factual_accuracy(response, reference),
            "logical_coherence": self.metrics.calculate_logical_coherence(response),
            "context_relevance": self.metrics.calculate_context_relevance(response, context)
        }
        
        return {
            "sample": sample,
            "response": response,
            "metrics": metrics,
            "type": sample_type
        }
    \end{lstlisting}

    \item \textbf{metrics.py} - Metric calculation implementation:
    \begin{lstlisting}[language=Python, breaklines=true, basicstyle=\ttfamily\scriptsize]
class FaithfulnessMetrics:
    def calculate_context_relevance(self, response: str, context: str) -> float:
        # Calculate semantic relevance
        semantic_relevance = self.calculate_semantic_similarity(response, context)
        
        # Calculate information coverage
        coverage_score = self.calculate_coverage(response, context)
        
        # Calculate topic consistency
        topic_score = self.calculate_topic_consistency(response, context)
        
        # Final weighted score
        final_score = (
            semantic_relevance * 0.4 +
            coverage_score * 0.3 +
            topic_score * 0.3
        )
        
        return float(final_score)
    \end{lstlisting}

    \item \textbf{report.py} - Report generation implementation:
    \begin{lstlisting}[language=Python, breaklines=true, basicstyle=\ttfamily\scriptsize]
class FaithfulnessReport:
    def generate_report(self, final_metrics, type_metrics, sample_results):
        # Generate visualizations
        self._generate_radar_charts()
        self._generate_heatmaps()
        self._generate_boxplots()
        self._generate_trend_plots()
        
        # Generate report content
        content = []
        content.append("# Evaluation Report\n")
        
        # Add overall metrics
        content.append("## 1. Overall Metrics")
        for metric, score in final_metrics.items():
            content.append(f"- {metric}: {score:.4f}")
        
        return "\n".join(content)
    \end{lstlisting}
\end{itemize}

\section{Evaluation Samples}

\subsection{Sample Types}
The framework includes diverse evaluation samples:

\begin{itemize}
    \item \textbf{Scientific Explanations}:
    \begin{itemize}
        \item Complex scientific concepts
        \item Technical processes
        \item Research findings
    \end{itemize}
    
    \item \textbf{Technical Analyses}:
    \begin{itemize}
        \item System architectures
        \item Algorithm explanations
        \item Performance evaluations
    \end{itemize}
    
    \item \textbf{Medical Advice}:
    \begin{itemize}
        \item Health recommendations
        \item Treatment explanations
        \item Medical procedures
    \end{itemize}
    
    \item \textbf{Historical Analyses}:
    \begin{itemize}
        \item Historical events
        \item Cultural developments
        \item Societal changes
    \end{itemize}
    
    \item \textbf{Current Events}:
    \begin{itemize}
        \item Recent developments
        \item Ongoing situations
        \item Contemporary issues
    \end{itemize}
\end{itemize}

\subsection{Sample Format}
Example of a sample in \texttt{samples.jsonl}:
\begin{lstlisting}[language=JSON, breaklines=true, basicstyle=\ttfamily\scriptsize]
{
    "id": "sci_001",
    "type": "scientific",
    "context": "Recent studies have shown that quantum entanglement...",
    "query": "Explain the concept of quantum entanglement.",
    "reference": "Quantum entanglement is a phenomenon where...",
    "metadata": {
        "domain": "physics",
        "complexity": "high",
        "required_background": "advanced"
    }
}
\end{lstlisting}

\section{Framework Configuration}

\subsection{Configuration File (faithfulness.yaml)}
The core configuration file defines evaluation parameters:

\begin{lstlisting}[language=YAML, breaklines=true, basicstyle=\ttfamily\scriptsize]
faithfulness_eval:
  id: faithfulness_eval.v1
  description: Framework for evaluating LLM response faithfulness
  metrics:
    - factual_accuracy
    - logical_coherence
    - context_relevance
    - interpretative_reasoning
    - information_completeness
    - hallucination_score
  
  weights:
    default:
      factual_accuracy: 0.25
      logical_coherence: 0.15
      context_relevance: 0.15
      interpretative_reasoning: 0.15
      information_completeness: 0.15
      hallucination_score: 0.15
    
    scientific:
      factual_accuracy: 0.35
      logical_coherence: 0.20
      context_relevance: 0.10
      interpretative_reasoning: 0.15
      information_completeness: 0.10
      hallucination_score: 0.10
\end{lstlisting}

\subsection{Dynamic Weight Adjustment Rules}
The framework implements dynamic weight adjustment based on evaluation results:

\begin{itemize}
    \item When factual accuracy is low (score < 0.5):
    \begin{itemize}
        \item Factual accuracy weight increases to 35\%
        \item Hallucination score weight increases to 20\%
        \item Other metrics share the remaining 45\%
    \end{itemize}
    
    \item When hallucination is severe (score < 0.5):
    \begin{itemize}
        \item Hallucination score weight increases to 25\%
        \item Factual accuracy weight increases to 30\%
        \item Other metrics share the remaining 45\%
    \end{itemize}
    \item Normal conditions maintain standard weights as defined in the configuration
\end{itemize}

\section{Project Structure}
The complete project structure:

\begin{lstlisting}[language=Text, breaklines=true, basicstyle=\ttfamily\scriptsize]
project_root/
|-- evals/
|   |-- elsuite/
|       |-- faithfulness/
|           |-- __init__.py
|           |-- eval.py
|           |-- metrics.py
|           |-- report.py
|           |-- run.py
|           |-- utils.py
|-- scripts/
|   |-- generate_visualization.py
|-- logs/
|   |-- faithfulness_eval_*.log
|-- results/
|   |-- faithfulness_eval_*/
|       |-- reports/
|           |-- final_metrics_{model_name}.json
|           |-- type_metrics_{model_name}.json
|           |-- sample_results_{model_name}.json
|           |-- report_{model_name}.md
|           |-- visualizations/
|               |-- overall_metrics_radar.png
|               |-- type_comparison.png
|               |-- metrics_heatmap.png
|               |-- metrics_boxplot.png
|               |-- metrics_trend.png
|-- visualizations/
|   |-- model_comparison.png
|   |-- type_comparison.png
|-- environment.yml
\end{lstlisting}
