\appendix
\section{Code Listings}

\subsection{Core Components}
The framework consists of several key Python modules that implement the core functionality:

\begin{itemize}
    \item \textbf{eval.py} - Core evaluation implementation that handles the main evaluation logic:
    \begin{lstlisting}[
        language=Python,
        breaklines=true,
        basicstyle=\ttfamily\footnotesize,
        commentstyle=\color{gray}
    ]
# Core evaluation class implementing the faithfulness metrics
class FaithfulnessEval(Eval):
    METRICS = [
        "factual_accuracy",
        "logical_coherence",
        "context_relevance",
        "interpretative_reasoning",
        "information_completeness",
        "hallucination_score"
    ]
    
    # Metric weights for different evaluation types
    TYPE_WEIGHTS = {
        "general": {
            "factual_accuracy": 0.30,
            "logical_coherence": 0.20,
            "context_relevance": 0.15,
            "interpretative_reasoning": 0.15,
            "information_completeness": 0.10,
            "hallucination_score": 0.10
        }
    }
    
    def eval_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        # Get key information from sample
        context = sample.get("context", "")
        query = sample.get("query", "")
        reference = sample.get("reference", "")
        sample_type = sample.get("type", "general")
        
        # Calculate metrics
        metrics = {
            "factual_accuracy": self.metrics.calculate_factual_accuracy(response, reference),
            "logical_coherence": self.metrics.calculate_logical_coherence(response),
            "context_relevance": self.metrics.calculate_context_relevance(response, context)
        }
        
        return {
            "sample": sample,
            "response": response,
            "metrics": metrics,
            "type": sample_type
        }
    \end{lstlisting}

    \vspace{0.5em}
    \item \textbf{metrics.py} - Implementation of individual metric calculations:
    \begin{lstlisting}[
        language=Python,
        breaklines=true,
        basicstyle=\ttfamily\footnotesize,
        commentstyle=\color{gray}
    ]
# Class implementing various faithfulness metrics
class FaithfulnessMetrics:
    def calculate_context_relevance(self, response: str, context: str) -> float:
        # Calculate semantic relevance between response and context
        semantic_relevance = self.calculate_semantic_similarity(response, context)
        
        # Calculate information coverage ratio
        coverage_score = self.calculate_coverage(response, context)
        
        # Calculate topic consistency score
        topic_score = self.calculate_topic_consistency(response, context)
        
        # Compute final weighted score
        final_score = (
            semantic_relevance * 0.40 +
            coverage_score * 0.30 +
            topic_score * 0.30
        )
        
        return float(final_score)
    \end{lstlisting}

    \vspace{0.5em}
    \item \textbf{report.py} - Report generation and visualization functionality:
    \begin{lstlisting}[
        language=Python,
        breaklines=true,
        basicstyle=\ttfamily\footnotesize,
        commentstyle=\color{gray}
    ]
# Class handling report generation and visualization
class FaithfulnessReport:
    def generate_report(self, final_metrics, type_metrics, sample_results):
        # Generate various visualization types
        self._generate_radar_charts()
        self._generate_heatmaps()
        self._generate_boxplots()
        self._generate_trend_plots()
        
        # Generate report content structure
        content = []
        content.append("# Evaluation Report\n")
        
        # Add overall metrics section
        content.append("## 1. Overall Metrics")
        for metric, score in final_metrics.items():
            content.append(f"- {metric}: {score:.2f}")
        
        return "\n".join(content)
    \end{lstlisting}
\end{itemize}

\vspace{0.5em}
\section{Evaluation Samples}

\subsection{Sample Types}
The framework includes the following diverse evaluation samples:

\begin{itemize}
    \item \textbf{Scientific Explanations}:
    \begin{itemize}
        \item Complex scientific concepts
        \item Technical processes
        \item Research findings
    \end{itemize}
    \vspace{0.5em}
    \item \textbf{Technical Analyses}:
    \begin{itemize}
        \item System architectures
        \item Algorithm explanations
        \item Performance evaluations
    \end{itemize}
    \vspace{0.5em}
    \item \textbf{Medical Advice}:
    \begin{itemize}
        \item Health recommendations
        \item Treatment explanations
        \item Medical procedures
    \end{itemize}
    \vspace{0.5em}
    \item \textbf{Historical Analyses}:
    \begin{itemize}
        \item Historical events
        \item Cultural developments
        \item Societal changes
    \end{itemize}
    \vspace{0.5em}
    \item \textbf{Current Events}:
    \begin{itemize}
        \item Recent developments
        \item Ongoing situations
        \item Contemporary issues
    \end{itemize}
\end{itemize}

\vspace{0.5em}
\subsection{Sample Format}
Example of a sample entry in the \texttt{samples.jsonl} file:

\begin{lstlisting}[
    language=JSON,
    breaklines=true,
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{gray}
]
{
    "id": "sci_001",
    "type": "scientific",
    "context": "Recent studies have shown that quantum entanglement...",
    "query": "Explain the concept of quantum entanglement.",
    "reference": "Quantum entanglement is a phenomenon where...",
    "metadata": {
        "domain": "physics",
        "complexity": "high",
        "required_background": "advanced"
    }
}
\end{lstlisting}

\section{Framework Configuration}

\subsection{Configuration File}
The core configuration file (\texttt{faithfulness.yaml}) defines the evaluation parameters:

\begin{lstlisting}[language=YAML, breaklines=true, basicstyle=\ttfamily\scriptsize, commentstyle=\color{gray}]
# Main configuration for faithfulness evaluation
faithfulness_eval:
  id: faithfulness_eval.v1
  description: Framework for evaluating LLM response faithfulness
  metrics:
    - factual_accuracy
    - logical_coherence
    - context_relevance
    - interpretative_reasoning
    - information_completeness
    - hallucination_score
  
  # Weight configurations for different evaluation types
  weights:
    default:
      factual_accuracy: 0.25
      logical_coherence: 0.15
      context_relevance: 0.15
      interpretative_reasoning: 0.15
      information_completeness: 0.15
      hallucination_score: 0.15
    
    scientific:
      factual_accuracy: 0.35
      logical_coherence: 0.20
      context_relevance: 0.10
      interpretative_reasoning: 0.15
      information_completeness: 0.10
      hallucination_score: 0.10
\end{lstlisting}

\subsection{Dynamic Weight Adjustment Rules}
The framework implements the following dynamic weight adjustment rules based on evaluation results:

\begin{itemize}
    \item When factual accuracy is low (score < 0.5):
    \begin{itemize}
        \item Factual accuracy weight increases to 35\%;
        \item Hallucination score weight increases to 20\%;
        \item Other metrics share the remaining 45\%.
    \end{itemize}
    
    \item When hallucination is severe (score < 0.5):
    \begin{itemize}
        \item Hallucination score weight increases to 25\%;
        \item Factual accuracy weight increases to 30\%;
        \item Other metrics share the remaining 45\%.
    \end{itemize}
    
    \item Under normal conditions:
    \begin{itemize}
        \item Standard weights are maintained as defined in the configuration file.
    \end{itemize}
\end{itemize}

\section{Project Structure}
The complete project structure is organized as follows:

\begin{lstlisting}[language=Text, breaklines=true, basicstyle=\ttfamily\scriptsize]
project_root/
|-- evals/
|   |-- elsuite/
|       |-- faithfulness/
|           |-- __init__.py
|           |-- eval.py
|           |-- metrics.py
|           |-- report.py
|           |-- run.py
|           |-- utils.py
|-- scripts/
|   |-- generate_visualization.py
|-- logs/
|   |-- faithfulness_eval_*.log
|-- results/
|   |-- faithfulness_eval_*/
|       |-- reports/
|           |-- final_metrics_{model_name}.json
|           |-- type_metrics_{model_name}.json
|           |-- sample_results_{model_name}.json
|           |-- report_{model_name}.md
|           |-- visualizations/
|               |-- overall_metrics_radar.png
|               |-- type_comparison.png
|               |-- metrics_heatmap.png
|               |-- metrics_boxplot.png
|               |-- metrics_trend.png
|-- visualizations/
|   |-- model_comparison.png
|   |-- type_comparison.png
|-- environment.yml
\end{lstlisting}
